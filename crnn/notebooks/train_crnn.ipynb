{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MRZ CRNN Training (v3: High Accuracy)\n\nMRZ (Machine Readable Zone) ç”¨ã®è»½é‡ CRNN ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã€‚\n\n## v3 æ”¹å–„ç‚¹\n- **Depthwise Separable Conv**: è¨ˆç®—é‡å‰Šæ¸› + æ–‡å­—å¹…æ„Ÿåº¦å‘ä¸Š\n- **ç¸¦æ¨ªæ¯”ä¿è­·**: æ‹¡å¼µæ™‚ã« 0/O ã®è­˜åˆ¥ç‰¹å¾´ã‚’ç¶­æŒ\n- **æ¨ªè§£åƒåº¦ã‚¢ãƒƒãƒ—**: 280 â†’ 320 px ã§æ–‡å­—å¹…ã®é•ã„ã‚’ä¿æŒ\n- **ãƒ‡ãƒ¼ã‚¿é‡5å€**: 10,000 â†’ 50,000 ã‚µãƒ³ãƒ—ãƒ«\n- **Curriculum Learning**: æ®µéšçš„ã«æ‹¡å¼µå¼·åº¦ã‚’ä¸Šã’ã‚‹\n- **æ··åŒæ–‡å­—é‡ç‚¹å­¦ç¿’**: 0/O, 1/I, 8/B ãªã©ã‚’å¤šãå«ã‚€ã‚µãƒ³ãƒ—ãƒ«\n\n## æ¦‚è¦\n- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: Depthwise Separable CNN + BiLSTM + Self-Attention + CTC\n- **å…¥åŠ›**: 32x320 ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒ\n- **å‡ºåŠ›**: 44æ–‡å­—ã® MRZ ãƒ†ã‚­ã‚¹ãƒˆ\n- **ç›®æ¨™**: CER < 0.1%, Accuracy >= 99%"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ç¢ºèª\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ Colab ã«äº‹å‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom pathlib import Path\nimport random\nimport time\n\n# cuDNNæœ€é©åŒ–ï¼ˆä½ãƒªã‚¹ã‚¯é«˜é€ŸåŒ–ï¼‰\ntorch.backends.cudnn.benchmark = True  # æœ€é©ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è‡ªå‹•é¸æŠ\ntorch.backends.cudnn.deterministic = False  # å†ç¾æ€§ã‚ˆã‚Šé€Ÿåº¦å„ªå…ˆ\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"cuDNN benchmark: {torch.backends.cudnn.benchmark}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. OCR-B ãƒ•ã‚©ãƒ³ãƒˆæº–å‚™\n\nMRZ ã¯ **OCR-B ãƒ•ã‚©ãƒ³ãƒˆ** ã§å°åˆ·ã•ã‚Œã¦ã„ã‚‹ï¼ˆå›½éš›æ¨™æº–ï¼‰ã€‚\nåˆæˆãƒ‡ãƒ¼ã‚¿ã‚‚å¿…ãš OCR-B ã‚’ä½¿ç”¨ã™ã‚‹ã€‚"
  },
  {
   "cell_type": "code",
   "source": "# OCR-B ãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆMRZ æ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆï¼‰\nimport urllib.request\nimport os\n\n# jsDelivr CDNçµŒç”±ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå®‰å®šã—ãŸã‚½ãƒ¼ã‚¹ï¼‰\n# å…ƒã‚½ãƒ¼ã‚¹: https://github.com/jaycee723/ocr-b\nOCRB_FONT_URL = \"https://cdn.jsdelivr.net/gh/jaycee723/ocr-b/dist/OCR-B.otf\"\nOCRB_FONT_PATH = \"/tmp/OCRB.otf\"\n\nif not os.path.exists(OCRB_FONT_PATH):\n    print(\"OCR-B ãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n    urllib.request.urlretrieve(OCRB_FONT_URL, OCRB_FONT_PATH)\n    print(f\"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {OCRB_FONT_PATH}\")\nelse:\n    print(f\"âœ… OCR-B ãƒ•ã‚©ãƒ³ãƒˆæº–å‚™æ¸ˆã¿: {OCRB_FONT_PATH}\")\n\n# ãƒ•ã‚©ãƒ³ãƒˆèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ\nfrom PIL import ImageFont\ntry:\n    test_font = ImageFont.truetype(OCRB_FONT_PATH, 24)\n    print(f\"âœ… ãƒ•ã‚©ãƒ³ãƒˆèª­ã¿è¾¼ã¿æˆåŠŸ\")\nexcept Exception as e:\n    raise RuntimeError(f\"OCR-B ãƒ•ã‚©ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance\nimport string\nimport io\n\n# MRZ ã§ä½¿ç”¨ã™ã‚‹æ–‡å­—ã‚»ãƒƒãƒˆ\nMRZ_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789<\"\n\n# æ··åŒã—ã‚„ã™ã„æ–‡å­—ãƒšã‚¢ï¼ˆv3: é‡ç‚¹å­¦ç¿’ç”¨ï¼‰\nCONFUSABLE_CHARS = \"0ODQ1IL8B5S\"\n\n\ndef generate_random_mrz_line(hard_mode: bool = False) -> str:\n    \"\"\"\n    ãƒ©ãƒ³ãƒ€ãƒ ãª MRZ è¡Œï¼ˆ44æ–‡å­—ï¼‰ã‚’ç”Ÿæˆ\n    \n    Args:\n        hard_mode: True ã®å ´åˆã€æ··åŒã—ã‚„ã™ã„æ–‡å­—ã‚’å¤šãå«ã‚€\n    \"\"\"\n    if hard_mode:\n        def pick_char(choices):\n            if random.random() < 0.3 and any(c in CONFUSABLE_CHARS for c in choices):\n                confusable = [c for c in choices if c in CONFUSABLE_CHARS]\n                if confusable:\n                    return random.choice(confusable)\n            return random.choice(choices)\n        pick_chars = lambda choices, k: \"\".join(pick_char(choices) for _ in range(k))\n    else:\n        pick_chars = lambda choices, k: \"\".join(random.choices(choices, k=k))\n    \n    if random.random() < 0.5:\n        doc_type = random.choice([\"P\", \"I\", \"A\", \"C\"])\n        country = pick_chars(string.ascii_uppercase, 3)\n        name_len = random.randint(20, 35)\n        name = pick_chars(string.ascii_uppercase + \"<\", name_len)\n        line = f\"{doc_type}<{country}{name}\"\n        line = line[:44].ljust(44, \"<\")\n    else:\n        passport_no = pick_chars(string.ascii_uppercase + string.digits, 9)\n        check1 = random.choice(string.digits)\n        nationality = pick_chars(string.ascii_uppercase, 3)\n        dob = pick_chars(string.digits, 6)\n        check2 = random.choice(string.digits)\n        sex = random.choice([\"M\", \"F\", \"<\"])\n        expiry = pick_chars(string.digits, 6)\n        check3 = random.choice(string.digits)\n        optional = pick_chars(string.ascii_uppercase + string.digits + \"<\", 14)\n        check4 = random.choice(string.digits)\n        line = f\"{passport_no}{check1}{nationality}{dob}{check2}{sex}{expiry}{check3}{optional}{check4}\"\n        line = line[:44]\n    \n    return line\n\n\ndef apply_augmentation(img: Image.Image, intensity: float = 1.0) -> Image.Image:\n    \"\"\"\n    ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆv3: ç¸¦æ¨ªæ¯”ä¿è­· + Curriculum Learningå¯¾å¿œï¼‰\n    \n    é‡è¦: OCR-B ãƒ•ã‚©ãƒ³ãƒˆã§ã¯ 0 ã¨ O ã®é•ã„ãŒç¸¦æ¨ªæ¯”ã§è¡¨ç¾ã•ã‚Œã‚‹ã€‚\n    æ‹¡å¼µæ™‚ã«ç¸¦æ¨ªæ¯”ã‚’æ­ªã‚ã‚‹ã¨ã€ã“ã®è­˜åˆ¥ç‰¹å¾´ãŒå¤±ã‚ã‚Œã‚‹ã€‚\n    â†’ ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã§ç­‰å€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨\n    \"\"\"\n    # ãƒ–ãƒ©ãƒ¼\n    if random.random() < 0.3 * intensity:\n        img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.3, 1.0)))\n    \n    # å›è»¢ï¼ˆç¸¦æ¨ªæ¯”ã‚’ä¿æŒï¼‰\n    if random.random() < 0.5 * intensity:\n        angle = random.uniform(-2, 2) * intensity\n        img = img.rotate(angle, fillcolor=255, resample=Image.BILINEAR)\n    \n    # ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ï¼ˆv3: ç¸¦æ¨ªæ¯”ã‚’ä¿è­·ï¼‰\n    if random.random() < 0.2 * intensity:\n        w, h = img.size\n        \n        # ç­‰å€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: x ã¨ y ã‚’åŒã˜å€ç‡ã§å¤‰æ›\n        # ã“ã‚Œã«ã‚ˆã‚Š 0(ç‹­ã„) ã¨ O(åºƒã„) ã®é•ã„ãŒä¿æŒã•ã‚Œã‚‹\n        scale = 1 + random.uniform(-0.02, 0.02) * intensity\n        \n        # ã›ã‚“æ–­ï¼ˆshearï¼‰ã¯æ¨ªæ–¹å‘ã®ã¿è¨±å®¹ï¼ˆæ–‡å­—å¹…ã«å½±éŸ¿ã—ã«ãã„ï¼‰\n        shear_x = random.uniform(-0.01, 0.01) * intensity\n        \n        # å¹³è¡Œç§»å‹•\n        tx = random.uniform(-0.02, 0.02) * w * intensity\n        ty = random.uniform(-0.02, 0.02) * h * intensity\n        \n        coeffs = [\n            scale,      # x ã‚¹ã‚±ãƒ¼ãƒ«\n            shear_x,    # x ã›ã‚“æ–­\n            tx,         # x å¹³è¡Œç§»å‹•\n            0,          # y ã›ã‚“æ–­ (0 ã§ç¸¦æ¨ªæ¯”ä¿è­·)\n            scale,      # y ã‚¹ã‚±ãƒ¼ãƒ« (x ã¨åŒã˜)\n            ty,         # y å¹³è¡Œç§»å‹•\n        ]\n        img = img.transform((w, h), Image.AFFINE, coeffs, fillcolor=255)\n    \n    # æ˜ã‚‹ã•\n    if random.random() < 0.4 * intensity:\n        factor = 1.0 + (random.uniform(-0.2, 0.2) * intensity)\n        img = ImageEnhance.Brightness(img).enhance(factor)\n    \n    # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ\n    if random.random() < 0.4 * intensity:\n        factor = 1.0 + (random.uniform(-0.2, 0.2) * intensity)\n        img = ImageEnhance.Contrast(img).enhance(factor)\n    \n    # JPEGåœ§ç¸®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ\n    if random.random() < 0.2 * intensity:\n        quality = int(95 - 25 * intensity)\n        buffer = io.BytesIO()\n        img.save(buffer, format='JPEG', quality=quality)\n        buffer.seek(0)\n        img = Image.open(buffer).convert('L')\n    \n    return img\n\n\ndef render_mrz_image(\n    text: str,\n    height: int = 32,\n    font_size: int = 24,\n    augment_intensity: float = 1.0\n) -> np.ndarray:\n    \"\"\"MRZ ãƒ†ã‚­ã‚¹ãƒˆã‚’ OCR-B ãƒ•ã‚©ãƒ³ãƒˆã§ç”»åƒã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\"\"\"\n    # OCR-B ãƒ•ã‚©ãƒ³ãƒˆã®ã¿ä½¿ç”¨ï¼ˆMRZæ¨™æº–ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ï¼‰\n    font = ImageFont.truetype(OCRB_FONT_PATH, font_size)\n    \n    dummy_img = Image.new(\"L\", (1, 1))\n    dummy_draw = ImageDraw.Draw(dummy_img)\n    bbox = dummy_draw.textbbox((0, 0), text, font=font)\n    text_width = bbox[2] - bbox[0]\n    text_height = bbox[3] - bbox[1]\n    \n    padding = 4\n    img_width = text_width + padding * 2\n    img = Image.new(\"L\", (img_width, height), color=255)\n    draw = ImageDraw.Draw(img)\n    \n    y_offset = (height - text_height) // 2\n    draw.text((padding, y_offset), text, font=font, fill=0)\n    \n    if augment_intensity > 0:\n        img = apply_augmentation(img, intensity=augment_intensity)\n    \n    img_array = np.array(img)\n    \n    if augment_intensity > 0 and random.random() < 0.6 * augment_intensity:\n        sigma = random.uniform(3, 10) * augment_intensity\n        noise = np.random.normal(0, sigma, img_array.shape)\n        img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)\n    \n    return img_array\n\n\n# ãƒ†ã‚¹ãƒˆ\nprint(\"v3: OCR-B + ç¸¦æ¨ªæ¯”ä¿è­· + Curriculum Learning + Hard Mode\")\nprint(\"\\né€šå¸¸ã‚µãƒ³ãƒ—ãƒ«:\")\nprint(f\"  {generate_random_mrz_line(hard_mode=False)}\")\nprint(\"\\næ··åŒæ–‡å­—é‡ç‚¹ã‚µãƒ³ãƒ—ãƒ«:\")\nhard = generate_random_mrz_line(hard_mode=True)\nprint(f\"  {hard}\")\nprint(f\"  æ··åŒæ–‡å­—æ•°: {sum(1 for c in hard if c in CONFUSABLE_CHARS)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’è¡¨ç¤º\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    text = generate_random_mrz_line()\n",
    "    img = render_mrz_image(text)\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(text, fontsize=8)\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import Dataset, DataLoader\n\n# æ–‡å­—ã‚»ãƒƒãƒˆå®šç¾©\nCHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789<\"\nCHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\nIDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\nNUM_CLASSES = len(CHARS) + 1  # +1 for CTC blank\n\nprint(f\"æ–‡å­—æ•°: {len(CHARS)}\")\nprint(f\"ã‚¯ãƒ©ã‚¹æ•° (blankå«ã‚€): {NUM_CLASSES}\")\n\n\ndef encode_text(text: str) -> list:\n    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›\"\"\"\n    return [CHAR_TO_IDX[c] for c in text if c in CHAR_TO_IDX]\n\n\ndef decode_output(indices: list) -> str:\n    \"\"\"\n    CTC å‡ºåŠ›ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ‡ã‚³ãƒ¼ãƒ‰\n    é€£ç¶šã™ã‚‹åŒä¸€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ blank ã‚’é™¤å»\n    \"\"\"\n    result = []\n    prev_idx = -1\n    for idx in indices:\n        if idx == len(CHARS):  # blank\n            prev_idx = idx\n            continue\n        if idx != prev_idx and idx < len(CHARS):\n            result.append(IDX_TO_CHAR[idx])\n        prev_idx = idx\n    return \"\".join(result)\n\n\nclass SyntheticMRZDataset(Dataset):\n    \"\"\"\n    åˆæˆ MRZ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (v3: Curriculum Learning + Hard Modeå¯¾å¿œ)\n    \n    ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ãƒ©ãƒ³ãƒ€ãƒ ã« MRZ ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€‚\n    - augment_intensity: æ‹¡å¼µå¼·åº¦ï¼ˆCurriculum Learningã§æ®µéšçš„ã«ä¸Šã’ã‚‹ï¼‰\n    - hard_ratio: æ··åŒæ–‡å­—é‡ç‚¹ã‚µãƒ³ãƒ—ãƒ«ã®å‰²åˆ\n    \"\"\"\n    \n    def __init__(\n        self, \n        num_samples: int, \n        max_width: int = 280,\n        augment_intensity: float = 1.0,\n        hard_ratio: float = 0.3\n    ):\n        self.num_samples = num_samples\n        self.max_width = max_width\n        self.augment_intensity = augment_intensity\n        self.hard_ratio = hard_ratio\n    \n    def set_augment_intensity(self, intensity: float):\n        \"\"\"Curriculum Learning ç”¨: æ‹¡å¼µå¼·åº¦ã‚’å‹•çš„ã«å¤‰æ›´\"\"\"\n        self.augment_intensity = intensity\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        # hard_ratio ã®ç¢ºç‡ã§æ··åŒæ–‡å­—é‡ç‚¹ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ\n        hard_mode = random.random() < self.hard_ratio\n        text = generate_random_mrz_line(hard_mode=hard_mode)\n        image = render_mrz_image(text, augment_intensity=self.augment_intensity)\n        \n        # æ­£è¦åŒ– (0-1)\n        image = image.astype(np.float32) / 255.0\n        \n        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆå¹…ã‚’ max_width ã«çµ±ä¸€ï¼‰\n        h, w = image.shape\n        if w < self.max_width:\n            pad_w = self.max_width - w\n            image = np.pad(image, ((0, 0), (0, pad_w)), constant_values=1.0)\n        elif w > self.max_width:\n            image = image[:, :self.max_width]\n        \n        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ› (1, H, W)\n        image_tensor = torch.from_numpy(image).unsqueeze(0)\n        label = encode_text(text)\n        \n        return {\n            \"image\": image_tensor,\n            \"label\": torch.tensor(label, dtype=torch.long),\n            \"label_length\": len(label),\n            \"text\": text\n        }\n\n\ndef collate_fn(batch):\n    \"\"\"ãƒãƒƒãƒã‚’ã¾ã¨ã‚ã‚‹ï¼ˆCTC Loss ç”¨ï¼‰\"\"\"\n    images = torch.stack([item[\"image\"] for item in batch])\n    labels = torch.cat([item[\"label\"] for item in batch])\n    label_lengths = torch.tensor([item[\"label_length\"] for item in batch])\n    texts = [item[\"text\"] for item in batch]\n    return {\n        \"images\": images,\n        \"labels\": labels,\n        \"label_lengths\": label_lengths,\n        \"texts\": texts\n    }\n\n\nprint(\"Dataset: v3 (Curriculum Learning + Hard Modeå¯¾å¿œ)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CRNN ãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DepthwiseSeparableConv(nn.Module):\n    \"\"\"\n    Depthwise Separable Convolution (MobileNet ã‚¹ã‚¿ã‚¤ãƒ«)\n    \n    é€šå¸¸ã® Conv2d ã‚’2ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£:\n    1. Depthwise Conv: å„ãƒãƒ£ãƒãƒ«ã‚’ç‹¬ç«‹ã«ç•³ã¿è¾¼ã¿ï¼ˆç©ºé–“ç‰¹å¾´æŠ½å‡ºï¼‰\n    2. Pointwise Conv: 1x1 ã§ãƒãƒ£ãƒãƒ«é–“ã‚’çµåˆ\n    \n    åŠ¹æœ:\n    - è¨ˆç®—é‡: ç´„ 1/8 ã«å‰Šæ¸›\n    - æ¨ªæ–¹å‘ã®ç‰¹å¾´ã‚’ä¿æŒã—ã‚„ã™ã„ï¼ˆæ–‡å­—å¹…ã®è­˜åˆ¥ã«æœ‰åˆ©ï¼‰\n    \"\"\"\n    \n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, \n                 stride: int = 1, padding: int = 1):\n        super().__init__()\n        \n        # Depthwise Conv: å„ãƒãƒ£ãƒãƒ«ã‚’ç‹¬ç«‹ã«å‡¦ç†\n        # groups=in_channels ã§å„ãƒãƒ£ãƒãƒ«ã«åˆ¥ã€…ã®ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨\n        self.depthwise = nn.Conv2d(\n            in_channels, in_channels, \n            kernel_size=kernel_size, \n            stride=stride, \n            padding=padding, \n            groups=in_channels,  # â† æ ¸å¿ƒ: ãƒãƒ£ãƒãƒ«ã”ã¨ã«ç‹¬ç«‹\n            bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        \n        # Pointwise Conv: 1x1 ã§ãƒãƒ£ãƒãƒ«é–“ã‚’çµåˆ\n        self.pointwise = nn.Conv2d(\n            in_channels, out_channels, \n            kernel_size=1, \n            bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        # Depthwise: ç©ºé–“æ–¹å‘ã®ç‰¹å¾´æŠ½å‡ºï¼ˆæ¨ªå¹…ã®é•ã„ã‚’ä¿æŒï¼‰\n        x = self.depthwise(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        # Pointwise: ãƒãƒ£ãƒãƒ«é–“ã®çµåˆ\n        x = self.pointwise(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        \n        return x\n\n\nclass EfficientCRNN(nn.Module):\n    \"\"\"\n    Efficient CRNN with Depthwise Separable Convolution (v3)\n    \n    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:\n    - Depthwise Separable CNN: è»½é‡ã‹ã¤æ¨ªæ–¹å‘ç‰¹å¾´ã‚’ä¿æŒ\n    - BiLSTM: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n    - Self-Attention: æ–‡å­—é–“ä¾å­˜é–¢ä¿‚\n    \n    v3 æ”¹å–„ç‚¹:\n    - Depthwise Separable Conv ã§è¨ˆç®—é‡å‰Šæ¸› + æ–‡å­—å¹…æ„Ÿåº¦å‘ä¸Š\n    - æœ€åˆã®å±¤ã¯é€šå¸¸ Convï¼ˆä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ã¯æ··ãœã‚‹æ–¹ãŒè‰¯ã„ï¼‰\n    - Squeeze-and-Excitation çš„ãªãƒãƒ£ãƒãƒ«æ³¨æ„ã‚’è¿½åŠ å¯èƒ½\n    \n    å…¥åŠ›: (B, 1, 32, W) - ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒ\n    å‡ºåŠ›: (T, B, 38) - å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®æ–‡å­—ç¢ºç‡\n    \"\"\"\n    \n    def __init__(self, num_classes: int = 38, hidden_size: int = 128, dropout: float = 0.2):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        \n        # CNN Backbone (Depthwise Separable)\n        self.cnn = nn.Sequential(\n            # Block 1: æœ€åˆã®å±¤ã¯é€šå¸¸ Convï¼ˆä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´æŠ½å‡ºï¼‰\n            # 32 -> 16\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            \n            # Block 2: Depthwise Separable (16 -> 8)\n            DepthwiseSeparableConv(32, 64),\n            nn.MaxPool2d(2, 2),\n            \n            # Block 3: Depthwise Separable (8 -> 4)\n            DepthwiseSeparableConv(64, 128),\n            nn.MaxPool2d((2, 1)),  # é«˜ã•ã®ã¿ç¸®å°ã€å¹…ã¯ä¿æŒ\n            \n            # Block 4: Depthwise Separable (4 -> 2)\n            DepthwiseSeparableConv(128, 256),\n            nn.MaxPool2d((2, 1)),  # é«˜ã•ã®ã¿ç¸®å°\n            \n            # Block 5: æœ€çµ‚å±¤ (2 -> 1)\n            nn.Conv2d(256, 256, kernel_size=(2, 1)),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n        )\n        \n        # BiLSTM\n        self.lstm = nn.LSTM(\n            input_size=256,\n            hidden_size=hidden_size,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=False,\n            dropout=dropout\n        )\n        \n        # Self-Attention\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_size * 2,\n            num_heads=4,\n            dropout=dropout,\n            batch_first=False\n        )\n        \n        # LayerNorm + Dropout\n        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n        self.dropout = nn.Dropout(dropout)\n        \n        # å‡ºåŠ›å±¤\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n    \n    def forward(self, x):\n        # CNN ç‰¹å¾´æŠ½å‡º: (B, 1, 32, W) -> (B, 256, 1, W')\n        features = self.cnn(x)\n        \n        # å½¢çŠ¶å¤‰æ›: (B, C, 1, W') -> (W', B, C) = (T, B, C)\n        b, c, h, w = features.shape\n        features = features.squeeze(2)\n        features = features.permute(2, 0, 1)  # (T, B, C)\n        \n        # BiLSTM\n        lstm_out, _ = self.lstm(features)\n        \n        # Self-Attention + Residual\n        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n        out = self.layer_norm(lstm_out + attn_out)\n        out = self.dropout(out)\n        \n        # å‡ºåŠ›\n        output = self.fc(out)\n        output = torch.log_softmax(output, dim=2)\n        \n        return output\n\n\n# æ—§ãƒ¢ãƒ‡ãƒ«ï¼ˆæ¯”è¼ƒç”¨ã«æ®‹ã™ï¼‰\nclass AttentionCRNN(nn.Module):\n    \"\"\"v2: é€šå¸¸ã® Conv2d + Attention\"\"\"\n    \n    def __init__(self, num_classes: int = 38, hidden_size: int = 128, dropout: float = 0.2):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d((2, 1)),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d((2, 1)),\n            nn.Conv2d(256, 256, kernel_size=(2, 1)), nn.BatchNorm2d(256), nn.ReLU(),\n        )\n        \n        self.lstm = nn.LSTM(256, hidden_size, 2, bidirectional=True, batch_first=False, dropout=dropout)\n        self.attention = nn.MultiheadAttention(hidden_size * 2, 4, dropout=dropout, batch_first=False)\n        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n    \n    def forward(self, x):\n        features = self.cnn(x)\n        b, c, h, w = features.shape\n        features = features.squeeze(2).permute(2, 0, 1)\n        lstm_out, _ = self.lstm(features)\n        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n        out = self.layer_norm(lstm_out + attn_out)\n        out = self.dropout(out)\n        output = self.fc(out)\n        return torch.log_softmax(output, dim=2)\n\n\ndef get_model_info(model):\n    \"\"\"ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n    size_mb = (param_size + buffer_size) / 1024 / 1024\n    return {\"total_params\": total_params, \"size_mb\": size_mb}\n\n\n# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\nprint(\"=\" * 50)\nprint(\"ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\")\nprint(\"=\" * 50)\n\n# v2: é€šå¸¸ Conv\nmodel_v2 = AttentionCRNN(num_classes=NUM_CLASSES, dropout=0.2)\ninfo_v2 = get_model_info(model_v2)\n\n# v3: Depthwise Separable Conv\nmodel_v3 = EfficientCRNN(num_classes=NUM_CLASSES, dropout=0.2)\ninfo_v3 = get_model_info(model_v3)\n\nprint(f\"AttentionCRNN (v2):\")\nprint(f\"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info_v2['total_params']:,}\")\nprint(f\"  ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {info_v2['size_mb']:.2f} MB\")\n\nprint(f\"\\nEfficientCRNN (v3 Depthwise Separable):\")\nprint(f\"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info_v3['total_params']:,}\")\nprint(f\"  ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {info_v3['size_mb']:.2f} MB\")\n\nreduction = (1 - info_v3['total_params'] / info_v2['total_params']) * 100\nprint(f\"\\nâ†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›: {reduction:.1f}%\")\n\n# æ¨è«–ãƒ†ã‚¹ãƒˆ\nx = torch.randn(1, 1, 32, 280)\noutput_v2 = model_v2(x)\noutput_v3 = model_v3(x)\nprint(f\"\\nå…¥åŠ›: {x.shape}\")\nprint(f\"å‡ºåŠ› (v2): {output_v2.shape}\")\nprint(f\"å‡ºåŠ› (v3): {output_v3.shape}\")\n\n# v3 ã‚’ä½¿ç”¨\nmodel = model_v3\nprint(f\"\\nâœ… EfficientCRNN (v3) ã‚’ä½¿ç”¨\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ç¿’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (v3: High Accuracy)\nBATCH_SIZE = 64   # å®‰å®šæ€§å„ªå…ˆ\nEPOCHS = 150      # 100 â†’ 150 (åæŸã¾ã§ã®ä½™è£•)\nLR = 1e-3\nWEIGHT_DECAY = 0.01\nTRAIN_SAMPLES = 50000   # 10,000 â†’ 50,000 (5å€)\nVAL_SAMPLES = 2000      # æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«ã‚‚å¢—åŠ \nMAX_WIDTH = 320         # 280 â†’ 320 (æ¨ªè§£åƒåº¦ã‚¢ãƒƒãƒ—: æ–‡å­—å¹…ã®é•ã„ã‚’ä¿æŒ)\n\n# Curriculum Learning ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nCURRICULUM_STAGES = [\n    # (é–‹å§‹epoch, augment_intensity, hard_ratio)\n    (1, 0.3, 0.1),    # Stage 1: ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ã§åŸºç¤å­¦ç¿’\n    (30, 0.6, 0.2),   # Stage 2: ä¸­ç¨‹åº¦ã®æ‹¡å¼µ\n    (60, 1.0, 0.3),   # Stage 3: ãƒ•ãƒ«æ‹¡å¼µ + æ··åŒæ–‡å­—é‡ç‚¹\n]\n\n# ãƒ‡ãƒã‚¤ã‚¹\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# num_workers: Colab/Kaggle ã§ã¯ 0 ãŒæœ€ã‚‚å®‰å®šï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç”Ÿæˆã®ãŸã‚ï¼‰\nNUM_WORKERS = 0\nprint(f\"num_workers: {NUM_WORKERS} (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç”Ÿæˆã®ãŸã‚)\")\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåˆæœŸã¯ä½å¼·åº¦ï¼‰\ninitial_intensity, initial_hard_ratio = CURRICULUM_STAGES[0][1], CURRICULUM_STAGES[0][2]\ntrain_dataset = SyntheticMRZDataset(\n    TRAIN_SAMPLES, MAX_WIDTH,\n    augment_intensity=initial_intensity,\n    hard_ratio=initial_hard_ratio\n)\nval_dataset = SyntheticMRZDataset(VAL_SAMPLES, MAX_WIDTH, augment_intensity=1.0, hard_ratio=0.3)\n\n# DataLoader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\nprint(f\"Train samples: {TRAIN_SAMPLES:,} (5x increase)\")\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\nprint(f\"MAX_WIDTH: {MAX_WIDTH} (æ¨ªè§£åƒåº¦ã‚¢ãƒƒãƒ—)\")\n\n# ãƒ¢ãƒ‡ãƒ« (v3: EfficientCRNN with Depthwise Separable Conv)\nmodel = EfficientCRNN(num_classes=NUM_CLASSES, dropout=0.2).to(device)\ninfo = get_model_info(model)\nprint(f\"\\nãƒ¢ãƒ‡ãƒ«: EfficientCRNN (Depthwise Separable)\")\nprint(f\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info['total_params']:,}\")\nprint(f\"ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {info['size_mb']:.2f} MB\")\nprint(\"â„¹ï¸ torch.compile() ç„¡åŠ¹ (Colab/Kaggle ç’°å¢ƒ)\")\n\ncriterion = nn.CTCLoss(blank=NUM_CLASSES - 1, zero_infinity=True)\noptimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# OneCycleLR: max_lr ã‚’æ§ãˆã‚ã«è¨­å®šï¼ˆCTC collapse é˜²æ­¢ï¼‰\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=LR * 3,  # 0.003 (CTC collapse é˜²æ­¢)\n    epochs=EPOCHS,\n    steps_per_epoch=len(train_loader),\n    pct_start=0.1,\n    anneal_strategy='cos'\n)\n\nprint(f\"\\n[v3 High Accuracy è¨­å®š]\")\nprint(f\"ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}\")\nprint(f\"ã‚¨ãƒãƒƒã‚¯æ•°: {EPOCHS}\")\nprint(f\"å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«: {TRAIN_SAMPLES:,}\")\nprint(f\"max_lr: {LR * 3:.4f}\")\nprint(f\"Curriculum Learning: æœ‰åŠ¹\")\nprint(f\"  Stage 1 (epoch 1-29):  intensity=0.3, hard_ratio=0.1\")\nprint(f\"  Stage 2 (epoch 30-59): intensity=0.6, hard_ratio=0.2\")\nprint(f\"  Stage 3 (epoch 60+):   intensity=1.0, hard_ratio=0.3\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PyTorch 2.x å¯¾å¿œã® AMP API\nfrom torch.amp import autocast, GradScaler\n\ndef train_epoch(model, dataloader, criterion, optimizer, scheduler, device, scaler):\n    \"\"\"\n    1ã‚¨ãƒãƒƒã‚¯åˆ†ã®å­¦ç¿’ï¼ˆAMPå¯¾å¿œã§é«˜é€ŸåŒ–ï¼‰\n    \n    Mixed Precision Training:\n    - forward passã‚’FP16ã§å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ– + ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰\n    - backward passã¯FP32ã§å®Ÿè¡Œï¼ˆç²¾åº¦ç¶­æŒï¼‰\n    - GradScalerã§gradientã®underflowé˜²æ­¢\n    \"\"\"\n    model.train()\n    total_loss = 0.0\n    \n    for batch in dataloader:\n        images = batch[\"images\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        label_lengths = batch[\"label_lengths\"]\n        \n        optimizer.zero_grad()\n        \n        # Mixed Precision: forward pass ã‚’FP16ã§å®Ÿè¡Œ\n        with autocast('cuda'):\n            outputs = model(images)  # (T, B, C)\n            T, B, C = outputs.shape\n            input_lengths = torch.full((B,), T, dtype=torch.long)\n            loss = criterion(outputs, labels, input_lengths, label_lengths)\n        \n        # Backward with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\n\ndef validate(model, dataloader, device):\n    \"\"\"æ¤œè¨¼\"\"\"\n    model.eval()\n    total_chars = 0\n    total_errors = 0\n    correct = 0\n    total = 0\n    samples = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            images = batch[\"images\"].to(device)\n            texts = batch[\"texts\"]\n            \n            with autocast('cuda'):\n                outputs = model(images)\n            \n            for i, text in enumerate(texts):\n                probs = outputs[:, i, :]\n                pred_indices = probs.argmax(dim=1).cpu().tolist()\n                pred_text = decode_output(pred_indices)\n                \n                errors = sum(1 for a, b in zip(text, pred_text) if a != b)\n                errors += abs(len(text) - len(pred_text))\n                total_chars += len(text)\n                total_errors += errors\n                \n                if text == pred_text:\n                    correct += 1\n                total += 1\n                \n                if len(samples) < 5:\n                    samples.append({\"gt\": text, \"pred\": pred_text, \"match\": text == pred_text})\n    \n    cer = (total_errors / total_chars) * 100 if total_chars > 0 else 0\n    accuracy = (correct / total) * 100 if total > 0 else 0\n    \n    return {\"cer\": cer, \"accuracy\": accuracy, \"samples\": samples}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å­¦ç¿’ãƒ«ãƒ¼ãƒ— (v3: Curriculum Learning + AMP)\nprint(\"=\" * 60)\nprint(\"å­¦ç¿’é–‹å§‹ (v3: Curriculum Learning + Mixed Precision)\")\nprint(\"=\" * 60)\n\n# GradScaler for AMP (PyTorch 2.x API)\nscaler = GradScaler('cuda')\n\nbest_cer = float(\"inf\")\nbest_epoch = 0\nVAL_FREQUENCY = 5  # 5ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«æ¤œè¨¼\n\nhistory = {\"train_loss\": [], \"val_cer\": [], \"val_acc\": [], \"lr\": []}\ncurrent_stage = 0  # Curriculum Learning ã®ã‚¹ãƒ†ãƒ¼ã‚¸è¿½è·¡\n\nstart_time = time.time()\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_start = time.time()\n    \n    # === Curriculum Learning: ã‚¹ãƒ†ãƒ¼ã‚¸æ›´æ–° ===\n    # ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ã«å¿œã˜ã¦æ‹¡å¼µå¼·åº¦ã¨æ··åŒæ–‡å­—æ¯”ç‡ã‚’èª¿æ•´\n    for stage_idx, (stage_epoch, intensity, hard_ratio) in enumerate(CURRICULUM_STAGES):\n        if epoch >= stage_epoch:\n            if stage_idx > current_stage:\n                current_stage = stage_idx\n                train_dataset.augment_intensity = intensity\n                train_dataset.hard_ratio = hard_ratio\n                print(f\"\\nğŸ“ˆ Curriculum Stage {stage_idx + 1}: intensity={intensity}, hard_ratio={hard_ratio}\")\n    \n    current_lr = optimizer.param_groups[0]['lr']\n    history[\"lr\"].append(current_lr)\n    \n    # å­¦ç¿’\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, scaler)\n    history[\"train_loss\"].append(train_loss)\n    \n    epoch_time = time.time() - epoch_start\n    \n    # æ¤œè¨¼ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ã”ã¨ or æœ€åˆã®10ã‚¨ãƒãƒƒã‚¯ or æœ€çµ‚ã‚¨ãƒãƒƒã‚¯ or ã‚¹ãƒ†ãƒ¼ã‚¸å¤‰æ›´ç›´å¾Œï¼‰\n    is_stage_change = any(epoch == s[0] for s in CURRICULUM_STAGES)\n    if epoch <= 10 or epoch % VAL_FREQUENCY == 0 or epoch == EPOCHS or is_stage_change:\n        val_result = validate(model, val_loader, device)\n        history[\"val_cer\"].append(val_result[\"cer\"])\n        history[\"val_acc\"].append(val_result[\"accuracy\"])\n        \n        stage_info = f\"[S{current_stage + 1}]\" if current_stage > 0 else \"\"\n        print(f\"Epoch {epoch:3d}/{EPOCHS} {stage_info} | \"\n              f\"Loss: {train_loss:.4f} | \"\n              f\"CER: {val_result['cer']:.2f}% | \"\n              f\"Acc: {val_result['accuracy']:.1f}% | \"\n              f\"LR: {current_lr:.2e} | \"\n              f\"Time: {epoch_time:.1f}s\")\n        \n        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜\n        if val_result[\"cer\"] < best_cer:\n            best_cer = val_result[\"cer\"]\n            best_epoch = epoch\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"cer\": best_cer,\n                \"accuracy\": val_result[\"accuracy\"]\n            }, \"best_model.pth\")\n            print(f\"  -> Best model saved (CER: {best_cer:.2f}%)\")\n        \n        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼ˆ30ã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰\n        if epoch % 30 == 0:\n            print(\"\\n  ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬:\")\n            for s in val_result[\"samples\"][:3]:\n                mark = \"âœ“\" if s[\"match\"] else \"âœ—\"\n                print(f\"    GT:   {s['gt']}\")\n                print(f\"    Pred: {s['pred']} [{mark}]\")\n            print()\n\ntotal_time = time.time() - start_time\nprint(\"\\n\" + \"=\" * 60)\nprint(\"å­¦ç¿’å®Œäº† (v3)\")\nprint(\"=\" * 60)\nprint(f\"ç·å­¦ç¿’æ™‚é–“: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†)\")\nprint(f\"ãƒ™ã‚¹ãƒˆ CER: {best_cer:.2f}% (Epoch {best_epoch})\")\nprint(f\"ç›®æ¨™: CER < 0.1%, Accuracy >= 99%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "checkpoint = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.training = False  # set to inference mode\n",
    "\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"CER: {checkpoint['cer']:.2f}%, Accuracy: {checkpoint['accuracy']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ & ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡ºåŠ›ã‚’ç¢ºä¿ï¼‰\nimport shutil\nimport os\n\n# ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\nmodel_cpu = model.cpu()\nmodel_cpu.eval()\ndummy_input = torch.randn(1, 1, 32, MAX_WIDTH)  # MAX_WIDTH ã‚’ä½¿ç”¨\n\ntorch.onnx.export(\n    model_cpu,\n    dummy_input,\n    \"mrz_crnn.onnx\",\n    input_names=[\"image\"],\n    output_names=[\"output\"],\n    dynamic_axes={\n        \"image\": {0: \"batch\", 3: \"width\"},\n        \"output\": {0: \"seq_len\", 1: \"batch\"}\n    },\n    opset_version=17,\n    dynamo=False\n)\n\nonnx_size = os.path.getsize(\"mrz_crnn.onnx\") / 1024 / 1024\nprint(f\"âœ… ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {onnx_size:.2f} MB\")\n\n# å³åº§ã«å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆKaggle/Colab ä¸¡å¯¾å¿œï¼‰\nIS_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE') is not None\n\nif IS_KAGGLE:\n    output_dir = \"/kaggle/working\"\n    shutil.copy(\"best_model.pth\", f\"{output_dir}/best_model.pth\")\n    shutil.copy(\"mrz_crnn.onnx\", f\"{output_dir}/mrz_crnn.onnx\")\n    print(f\"âœ… Kaggle Output ã«ä¿å­˜å®Œäº†\")\nelse:\n    print(f\"âœ… ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜æ¸ˆã¿\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n\nColab: Google Driveã«ä¿å­˜  \nKaggle: Output ã¨ã—ã¦ä¿å­˜ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ï¼‰"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. å®Œäº†\n\nãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:\n- `best_model.pth`: PyTorch ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ\n- `mrz_crnn.onnx`: ONNX ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶æ¨è«–ç”¨ï¼‰\n\nKaggle: Output ã‚¿ãƒ–ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}