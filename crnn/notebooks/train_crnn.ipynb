{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MRZ CRNN Training (v2: Enhanced)\n\nMRZ (Machine Readable Zone) 用の軽量 CRNN モデルを学習する。\n\n## v2 改善点\n- **データ拡張強化**: ノイズ、ブラー、回転、Perspective変換、輝度変動\n- **Attention機構追加**: Self-Attention で文字間依存関係を学習\n- **正則化強化**: Dropout増加、Weight Decay調整\n\n## 概要\n- **アーキテクチャ**: CNN + BiLSTM + **Self-Attention** + CTC\n- **入力**: 32x280 グレースケール画像\n- **出力**: 44文字の MRZ テキスト\n- **目標**: CER < 0.5%, Accuracy >= 99%"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 確認\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 必要なライブラリは Colab に事前インストール済み\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom pathlib import Path\nimport random\nimport time\n\n# cuDNN最適化（低リスク高速化）\ntorch.backends.cudnn.benchmark = True  # 最適なアルゴリズム自動選択\ntorch.backends.cudnn.deterministic = False  # 再現性より速度優先\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"cuDNN benchmark: {torch.backends.cudnn.benchmark}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. OCR-B フォント準備 & 合成 MRZ データ生成\n\nMRZ は **OCR-B フォント** で印刷されている。\n合成データも OCR-B を使用して実際の MRZ に近い画像を生成する。"
  },
  {
   "cell_type": "code",
   "source": "from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance\nimport string\nimport io\n\n# MRZ で使用する文字セット\nMRZ_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789<\"\n\ndef generate_random_mrz_line() -> str:\n    \"\"\"\n    ランダムな MRZ 行（44文字）を生成\n    \n    実際の MRZ フォーマットに近いパターンを生成:\n    - Line 1: P<COUNTRY_CODE + NAME + FILLER\n    - Line 2: PASSPORT_NO + CHECK + NATIONALITY + DOB + CHECK + SEX + EXPIRY + CHECK + OPTIONAL + CHECK\n    \"\"\"\n    if random.random() < 0.5:\n        # Line 1 形式\n        doc_type = random.choice([\"P\", \"I\", \"A\", \"C\"])\n        country = \"\".join(random.choices(string.ascii_uppercase, k=3))\n        name_len = random.randint(20, 35)\n        name = \"\".join(random.choices(string.ascii_uppercase + \"<\", k=name_len))\n        line = f\"{doc_type}<{country}{name}\"\n        line = line[:44].ljust(44, \"<\")\n    else:\n        # Line 2 形式\n        passport_no = \"\".join(random.choices(string.ascii_uppercase + string.digits, k=9))\n        check1 = random.choice(string.digits)\n        nationality = \"\".join(random.choices(string.ascii_uppercase, k=3))\n        dob = \"\".join(random.choices(string.digits, k=6))\n        check2 = random.choice(string.digits)\n        sex = random.choice([\"M\", \"F\", \"<\"])\n        expiry = \"\".join(random.choices(string.digits, k=6))\n        check3 = random.choice(string.digits)\n        optional = \"\".join(random.choices(string.ascii_uppercase + string.digits + \"<\", k=14))\n        check4 = random.choice(string.digits)\n        line = f\"{passport_no}{check1}{nationality}{dob}{check2}{sex}{expiry}{check3}{optional}{check4}\"\n        line = line[:44]\n    \n    return line\n\n\ndef apply_augmentation(img: Image.Image) -> Image.Image:\n    \"\"\"\n    データ拡張を適用（v2 強化版）\n    \n    実際のスキャン/カメラ撮影で発生する劣化をシミュレート:\n    - ガウシアンブラー: フォーカスぼけ\n    - 回転: 微小な傾き\n    - Perspective変換: 斜め撮影\n    - 輝度/コントラスト変動: 照明条件\n    - ノイズ: センサーノイズ\n    - JPEG圧縮: 圧縮アーティファクト\n    \"\"\"\n    # 1. ガウシアンブラー (30%の確率)\n    if random.random() < 0.3:\n        radius = random.uniform(0.3, 1.0)\n        img = img.filter(ImageFilter.GaussianBlur(radius=radius))\n    \n    # 2. 回転 (50%の確率、±2度)\n    if random.random() < 0.5:\n        angle = random.uniform(-2, 2)\n        img = img.rotate(angle, fillcolor=255, resample=Image.BILINEAR)\n    \n    # 3. Perspective変換 (20%の確率)\n    if random.random() < 0.2:\n        w, h = img.size\n        dx = random.uniform(-0.02, 0.02) * w\n        dy = random.uniform(-0.02, 0.02) * h\n        coeffs = [\n            1 + random.uniform(-0.01, 0.01),\n            random.uniform(-0.02, 0.02),\n            dx,\n            random.uniform(-0.02, 0.02),\n            1 + random.uniform(-0.01, 0.01),\n            dy,\n            0, 0\n        ]\n        img = img.transform((w, h), Image.AFFINE, coeffs[:6], fillcolor=255)\n    \n    # 4. 輝度変動 (40%の確率)\n    if random.random() < 0.4:\n        factor = random.uniform(0.8, 1.2)\n        enhancer = ImageEnhance.Brightness(img)\n        img = enhancer.enhance(factor)\n    \n    # 5. コントラスト変動 (40%の確率)\n    if random.random() < 0.4:\n        factor = random.uniform(0.8, 1.2)\n        enhancer = ImageEnhance.Contrast(img)\n        img = enhancer.enhance(factor)\n    \n    # 6. JPEG圧縮シミュレーション (20%の確率)\n    if random.random() < 0.2:\n        quality = random.randint(70, 95)\n        buffer = io.BytesIO()\n        img.save(buffer, format='JPEG', quality=quality)\n        buffer.seek(0)\n        img = Image.open(buffer).convert('L')\n    \n    return img\n\n\ndef render_mrz_image(\n    text: str,\n    height: int = 32,\n    font_size: int = 24,\n    augment: bool = True\n) -> np.ndarray:\n    \"\"\"\n    MRZ テキストを OCR-B フォントで画像にレンダリング（v2 強化版）\n    \n    Args:\n        text: 44文字の MRZ テキスト\n        height: 出力画像の高さ\n        font_size: フォントサイズ\n        augment: データ拡張を適用するか\n    \n    Returns:\n        グレースケール画像 (H, W)\n    \"\"\"\n    # OCR-B フォントを使用（MRZ標準フォント）\n    try:\n        font = ImageFont.truetype(OCRB_FONT_PATH, font_size)\n    except:\n        # フォールバック: モノスペースフォント\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf\", font_size)\n        except:\n            font = ImageFont.load_default()\n    \n    # テキストサイズを計算\n    dummy_img = Image.new(\"L\", (1, 1))\n    dummy_draw = ImageDraw.Draw(dummy_img)\n    bbox = dummy_draw.textbbox((0, 0), text, font=font)\n    text_width = bbox[2] - bbox[0]\n    text_height = bbox[3] - bbox[1]\n    \n    # 画像を作成（白背景）\n    padding = 4\n    img_width = text_width + padding * 2\n    img = Image.new(\"L\", (img_width, height), color=255)\n    draw = ImageDraw.Draw(img)\n    \n    # テキストを描画（黒文字）\n    y_offset = (height - text_height) // 2\n    draw.text((padding, y_offset), text, font=font, fill=0)\n    \n    # データ拡張を適用\n    if augment:\n        img = apply_augmentation(img)\n    \n    # NumPy 配列に変換\n    img_array = np.array(img)\n    \n    # ガウシアンノイズ (60%の確率)\n    if augment and random.random() < 0.6:\n        sigma = random.uniform(3, 10)\n        noise = np.random.normal(0, sigma, img_array.shape)\n        img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)\n    \n    return img_array\n\n\n# テスト\nprint(\"データ拡張テスト (v2 強化版 + OCR-B フォント)\")\nsample_text = generate_random_mrz_line()\nsample_img = render_mrz_image(sample_text, augment=True)\nprint(f\"Sample MRZ: {sample_text}\")\nprint(f\"Image shape: {sample_img.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプル画像を表示\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    text = generate_random_mrz_line()\n",
    "    img = render_mrz_image(text)\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(text, fontsize=8)\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import Dataset, DataLoader\n\n# 文字セット定義\nCHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789<\"\nCHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\nIDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\nNUM_CLASSES = len(CHARS) + 1  # +1 for CTC blank\n\nprint(f\"文字数: {len(CHARS)}\")\nprint(f\"クラス数 (blank含む): {NUM_CLASSES}\")\n\n\ndef encode_text(text: str) -> list:\n    \"\"\"テキストを数値インデックスに変換\"\"\"\n    return [CHAR_TO_IDX[c] for c in text if c in CHAR_TO_IDX]\n\n\ndef decode_output(indices: list) -> str:\n    \"\"\"\n    CTC 出力をテキストにデコード\n    連続する同一インデックスと blank を除去\n    \"\"\"\n    result = []\n    prev_idx = -1\n    for idx in indices:\n        if idx == len(CHARS):  # blank\n            prev_idx = idx\n            continue\n        if idx != prev_idx and idx < len(CHARS):\n            result.append(IDX_TO_CHAR[idx])\n        prev_idx = idx\n    return \"\".join(result)\n\n\nclass SyntheticMRZDataset(Dataset):\n    \"\"\"\n    合成 MRZ データセット (v2)\n    \n    オンラインでランダムに MRZ 画像を生成する。\n    epoch ごとに異なるデータが生成される。\n    OCR-B フォント + データ拡張強化。\n    \"\"\"\n    \n    def __init__(self, num_samples: int, max_width: int = 280):\n        self.num_samples = num_samples\n        self.max_width = max_width\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        # ランダムな MRZ を生成\n        text = generate_random_mrz_line()\n        image = render_mrz_image(text, augment=True)  # v2: augment引数\n        \n        # 正規化 (0-1)\n        image = image.astype(np.float32) / 255.0\n        \n        # パディング（幅を max_width に統一）\n        h, w = image.shape\n        if w < self.max_width:\n            pad_w = self.max_width - w\n            image = np.pad(image, ((0, 0), (0, pad_w)), constant_values=1.0)\n        elif w > self.max_width:\n            image = image[:, :self.max_width]\n        \n        # テンソルに変換 (1, H, W)\n        image_tensor = torch.from_numpy(image).unsqueeze(0)\n        label = encode_text(text)\n        \n        return {\n            \"image\": image_tensor,\n            \"label\": torch.tensor(label, dtype=torch.long),\n            \"label_length\": len(label),\n            \"text\": text\n        }\n\n\ndef collate_fn(batch):\n    \"\"\"バッチをまとめる（CTC Loss 用）\"\"\"\n    images = torch.stack([item[\"image\"] for item in batch])\n    labels = torch.cat([item[\"label\"] for item in batch])\n    label_lengths = torch.tensor([item[\"label_length\"] for item in batch])\n    texts = [item[\"text\"] for item in batch]\n    return {\n        \"images\": images,\n        \"labels\": labels,\n        \"label_lengths\": label_lengths,\n        \"texts\": texts\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CRNN モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class AttentionCRNN(nn.Module):\n    \"\"\"\n    Attention-enhanced CRNN for MRZ OCR (v2)\n    \n    アーキテクチャ:\n    - CNN Backbone: 特徴抽出（高さを1に圧縮）\n    - BiLSTM: シーケンスモデリング\n    - Self-Attention: 文字間依存関係の学習（類似文字 0/O, 1/I の識別向上）\n    - Linear: 文字分類（37クラス + CTC blank）\n    \n    入力: (B, 1, 32, W) - グレースケール画像\n    出力: (T, B, 38) - 各タイムステップの文字確率\n    \n    v2 改善点:\n    - Self-Attention追加（num_heads=4）\n    - Dropout強化（0.1 → 0.2）\n    - LayerNorm追加で学習安定化\n    \"\"\"\n    \n    def __init__(self, num_classes: int = 38, hidden_size: int = 128, dropout: float = 0.2):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        \n        # CNN Backbone（変更なし）\n        self.cnn = nn.Sequential(\n            # Block 1: 32 -> 16\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            \n            # Block 2: 16 -> 8\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            \n            # Block 3: 8 -> 4\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 1)),\n            \n            # Block 4: 4 -> 2\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 1)),\n            \n            # Block 5: 2 -> 1\n            nn.Conv2d(256, 256, kernel_size=(2, 1)),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n        )\n        \n        # BiLSTM（Dropout強化）\n        self.lstm = nn.LSTM(\n            input_size=256,\n            hidden_size=hidden_size,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=False,\n            dropout=dropout\n        )\n        \n        # Self-Attention (v2 追加)\n        # BiLSTM出力に対してSelf-Attentionを適用\n        # 文字間の依存関係を学習し、類似文字の識別精度を向上\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_size * 2,  # BiLSTM出力は hidden_size * 2\n            num_heads=4,\n            dropout=dropout,\n            batch_first=False  # (T, B, C) 形式\n        )\n        \n        # LayerNorm（Attention後の正規化で学習安定化）\n        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n        \n        # Dropout層\n        self.dropout = nn.Dropout(dropout)\n        \n        # 出力層\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n    \n    def forward(self, x):\n        # CNN 特徴抽出: (B, 1, 32, W) -> (B, 256, 1, W')\n        features = self.cnn(x)\n        \n        # 形状変換: (B, C, 1, W') -> (W', B, C) = (T, B, C)\n        b, c, h, w = features.shape\n        features = features.squeeze(2)\n        features = features.permute(2, 0, 1)  # (T, B, C)\n        \n        # BiLSTM: (T, B, 256) -> (T, B, hidden*2)\n        lstm_out, _ = self.lstm(features)\n        \n        # Self-Attention: (T, B, C) -> (T, B, C)\n        # 文字間の依存関係を学習\n        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n        \n        # Residual connection + LayerNorm\n        # 元のLSTM出力とAttention出力を足し合わせて安定化\n        out = self.layer_norm(lstm_out + attn_out)\n        out = self.dropout(out)\n        \n        # 出力層: (T, B, hidden*2) -> (T, B, num_classes)\n        output = self.fc(out)\n        output = torch.log_softmax(output, dim=2)\n        \n        return output\n\n\n# 旧モデルも残しておく（比較用）\nclass CRNN(nn.Module):\n    \"\"\"旧バージョン（Attentionなし）\"\"\"\n    def __init__(self, num_classes: int = 38, hidden_size: int = 128):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d((2, 1)),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d((2, 1)),\n            nn.Conv2d(256, 256, kernel_size=(2, 1)), nn.BatchNorm2d(256), nn.ReLU(),\n        )\n        self.lstm = nn.LSTM(256, hidden_size, 2, bidirectional=True, batch_first=False, dropout=0.1)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n    \n    def forward(self, x):\n        features = self.cnn(x)\n        b, c, h, w = features.shape\n        features = features.squeeze(2).permute(2, 0, 1)\n        lstm_out, _ = self.lstm(features)\n        output = self.fc(lstm_out)\n        return torch.log_softmax(output, dim=2)\n\n\ndef get_model_info(model):\n    \"\"\"モデル情報を取得\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n    size_mb = (param_size + buffer_size) / 1024 / 1024\n    return {\"total_params\": total_params, \"size_mb\": size_mb}\n\n\n# AttentionCRNN（v2）を使用\nmodel = AttentionCRNN(num_classes=NUM_CLASSES, dropout=0.2)\ninfo = get_model_info(model)\nprint(f\"モデル: AttentionCRNN (v2)\")\nprint(f\"パラメータ数: {info['total_params']:,}\")\nprint(f\"モデルサイズ: {info['size_mb']:.2f} MB\")\n\n# 推論テスト\nx = torch.randn(1, 1, 32, 280)\noutput = model(x)\nprint(f\"入力: {x.shape} -> 出力: {output.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n# ハイパーパラメータ (v2)\nBATCH_SIZE = 64\nEPOCHS = 100\nLR = 1e-3\nWEIGHT_DECAY = 0.01  # v2: 正則化強化\nTRAIN_SAMPLES = 10000\nVAL_SAMPLES = 1000\nMAX_WIDTH = 280\n\n# デバイス\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# データセット\ntrain_dataset = SyntheticMRZDataset(TRAIN_SAMPLES, MAX_WIDTH)\nval_dataset = SyntheticMRZDataset(VAL_SAMPLES, MAX_WIDTH)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=2\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=2\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\n\n# モデル・損失関数・オプティマイザ (v2: AttentionCRNN使用)\nmodel = AttentionCRNN(num_classes=NUM_CLASSES, dropout=0.2).to(device)\ncriterion = nn.CTCLoss(blank=NUM_CLASSES - 1, zero_infinity=True)\noptimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# OneCycleLR: Warmup → 高学習率 → 低学習率\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=LR * 10,           # 最大学習率 1e-2\n    epochs=EPOCHS,\n    steps_per_epoch=len(train_loader),\n    pct_start=0.1,            # 最初の10%で warmup\n    anneal_strategy='cos'\n)\n\nprint(f\"\\n[v2 設定]\")\nprint(f\"モデル: AttentionCRNN (Self-Attention + Dropout 0.2)\")\nprint(f\"Weight Decay: {WEIGHT_DECAY}\")\nprint(f\"データ拡張: ノイズ、ブラー、回転、Perspective、輝度変動、JPEG圧縮\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n# ハイパーパラメータ (v2 高速化版)\nBATCH_SIZE = 128  # 64 → 128 (GPU活用率向上)\nEPOCHS = 100\nLR = 1e-3\nWEIGHT_DECAY = 0.01\nTRAIN_SAMPLES = 10000\nVAL_SAMPLES = 1000\nMAX_WIDTH = 280\n\n# デバイス\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# データセット\ntrain_dataset = SyntheticMRZDataset(TRAIN_SAMPLES, MAX_WIDTH)\nval_dataset = SyntheticMRZDataset(VAL_SAMPLES, MAX_WIDTH)\n\n# DataLoader (高速化設定)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=4,      # 2 → 4\n    pin_memory=True,    # GPU転送高速化\n    persistent_workers=True  # ワーカー再利用\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=4,\n    pin_memory=True,\n    persistent_workers=True\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\n\n# モデル\nmodel = AttentionCRNN(num_classes=NUM_CLASSES, dropout=0.2).to(device)\n\n# torch.compile() で高速化 (PyTorch 2.0+)\nif hasattr(torch, 'compile'):\n    model = torch.compile(model)\n    print(\"✅ torch.compile() 有効化\")\n\ncriterion = nn.CTCLoss(blank=NUM_CLASSES - 1, zero_infinity=True)\noptimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=LR * 10,\n    epochs=EPOCHS,\n    steps_per_epoch=len(train_loader),\n    pct_start=0.1,\n    anneal_strategy='cos'\n)\n\nprint(f\"\\n[v2 高速化設定]\")\nprint(f\"バッチサイズ: {BATCH_SIZE}\")\nprint(f\"num_workers: 4, pin_memory: True\")\nprint(f\"Mixed Precision (AMP): 有効\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.cuda.amp import autocast, GradScaler\n\ndef train_epoch(model, dataloader, criterion, optimizer, scheduler, device, scaler):\n    \"\"\"\n    1エポック分の学習（AMP対応で高速化）\n    \n    Mixed Precision Training:\n    - forward passをFP16で実行（高速化 + メモリ削減）\n    - backward passはFP32で実行（精度維持）\n    - GradScalerでgradientのunderflow防止\n    \"\"\"\n    model.train()\n    total_loss = 0.0\n    \n    for batch in dataloader:\n        images = batch[\"images\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        label_lengths = batch[\"label_lengths\"]\n        \n        optimizer.zero_grad()\n        \n        # Mixed Precision: forward pass をFP16で実行\n        with autocast():\n            outputs = model(images)  # (T, B, C)\n            T, B, C = outputs.shape\n            input_lengths = torch.full((B,), T, dtype=torch.long)\n            loss = criterion(outputs, labels, input_lengths, label_lengths)\n        \n        # Backward with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\n\ndef validate(model, dataloader, device):\n    \"\"\"検証\"\"\"\n    model.eval()\n    total_chars = 0\n    total_errors = 0\n    correct = 0\n    total = 0\n    samples = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            images = batch[\"images\"].to(device)\n            texts = batch[\"texts\"]\n            \n            with autocast():\n                outputs = model(images)\n            \n            for i, text in enumerate(texts):\n                probs = outputs[:, i, :]\n                pred_indices = probs.argmax(dim=1).cpu().tolist()\n                pred_text = decode_output(pred_indices)\n                \n                errors = sum(1 for a, b in zip(text, pred_text) if a != b)\n                errors += abs(len(text) - len(pred_text))\n                total_chars += len(text)\n                total_errors += errors\n                \n                if text == pred_text:\n                    correct += 1\n                total += 1\n                \n                if len(samples) < 5:\n                    samples.append({\"gt\": text, \"pred\": pred_text, \"match\": text == pred_text})\n    \n    cer = (total_errors / total_chars) * 100 if total_chars > 0 else 0\n    accuracy = (correct / total) * 100 if total > 0 else 0\n    \n    return {\"cer\": cer, \"accuracy\": accuracy, \"samples\": samples}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 学習ループ (AMP + Early Stopping + 検証頻度削減)\nprint(\"=\" * 60)\nprint(\"学習開始 (Mixed Precision + Early Stopping)\")\nprint(\"=\" * 60)\n\n# GradScaler for AMP\nscaler = GradScaler()\n\nbest_cer = float(\"inf\")\nbest_epoch = 0\nEARLY_STOP_PATIENCE = 15  # 15エポック改善なしで終了\nVAL_FREQUENCY = 5  # 5エポックごとに検証\n\nhistory = {\"train_loss\": [], \"val_cer\": [], \"val_acc\": [], \"lr\": []}\n\nstart_time = time.time()\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_start = time.time()\n    \n    current_lr = optimizer.param_groups[0]['lr']\n    history[\"lr\"].append(current_lr)\n    \n    # 学習\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, scaler)\n    history[\"train_loss\"].append(train_loss)\n    \n    epoch_time = time.time() - epoch_start\n    \n    # 検証（5エポックごと or 最初の10エポック or 最終エポック）\n    if epoch <= 10 or epoch % VAL_FREQUENCY == 0 or epoch == EPOCHS:\n        val_result = validate(model, val_loader, device)\n        history[\"val_cer\"].append(val_result[\"cer\"])\n        history[\"val_acc\"].append(val_result[\"accuracy\"])\n        \n        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n              f\"Loss: {train_loss:.4f} | \"\n              f\"CER: {val_result['cer']:.2f}% | \"\n              f\"Acc: {val_result['accuracy']:.1f}% | \"\n              f\"LR: {current_lr:.2e} | \"\n              f\"Time: {epoch_time:.1f}s\")\n        \n        # ベストモデル保存\n        if val_result[\"cer\"] < best_cer:\n            best_cer = val_result[\"cer\"]\n            best_epoch = epoch\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"cer\": best_cer,\n                \"accuracy\": val_result[\"accuracy\"]\n            }, \"best_model.pth\")\n            print(f\"  -> Best model saved (CER: {best_cer:.2f}%)\")\n        \n        # サンプル表示（20エポックごと）\n        if epoch % 20 == 0:\n            print(\"\\n  サンプル予測:\")\n            for s in val_result[\"samples\"][:3]:\n                mark = \"OK\" if s[\"match\"] else \"NG\"\n                print(f\"    GT:   {s['gt']}\")\n                print(f\"    Pred: {s['pred']} [{mark}]\")\n            print()\n        \n        # Early Stopping チェック\n        if epoch - best_epoch >= EARLY_STOP_PATIENCE:\n            print(f\"\\n⚡ Early Stopping: {EARLY_STOP_PATIENCE}エポック改善なし\")\n            break\n    else:\n        # 検証スキップ時は簡易ログ\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch:3d}/{EPOCHS} | Loss: {train_loss:.4f} | Time: {epoch_time:.1f}s (検証スキップ)\")\n\ntotal_time = time.time() - start_time\nprint(\"\\n\" + \"=\" * 60)\nprint(\"学習完了\")\nprint(\"=\" * 60)\nprint(f\"総学習時間: {total_time:.1f}秒 ({total_time/60:.1f}分)\")\nprint(f\"ベスト CER: {best_cer:.2f}% (Epoch {best_epoch})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ONNX エクスポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベストモデルをロード\n",
    "checkpoint = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.training = False  # set to inference mode\n",
    "\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"CER: {checkpoint['cer']:.2f}%, Accuracy: {checkpoint['accuracy']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 依存ライブラリをインストール（ONNX エクスポート用）\n!pip install onnx onnxscript onnxruntime -q\n\n# ONNX エクスポート（dynamo=False で旧エクスポーター使用）\n# PyTorch 2.9+ の dynamo エクスポーターに BatchNorm 関連のバグがあるため\nmodel_cpu = model.cpu()\nmodel_cpu.training = False\ndummy_input = torch.randn(1, 1, 32, 280)\n\ntorch.onnx.export(\n    model_cpu,\n    dummy_input,\n    \"mrz_crnn.onnx\",\n    input_names=[\"image\"],\n    output_names=[\"output\"],\n    dynamic_axes={\n        \"image\": {0: \"batch\", 3: \"width\"},\n        \"output\": {0: \"seq_len\", 1: \"batch\"}\n    },\n    opset_version=17,\n    dynamo=False  # 旧エクスポーター使用（BatchNorm バグ回避）\n)\n\nimport os\nonnx_size = os.path.getsize(\"mrz_crnn.onnx\") / 1024 / 1024\nprint(f\"ONNX モデルサイズ: {onnx_size:.2f} MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ONNX モデルの検証\nimport onnxruntime as ort\n\nsession = ort.InferenceSession(\"mrz_crnn.onnx\")\n\n# テスト推論\ntest_input = np.random.randn(1, 1, 32, 280).astype(np.float32)\noutputs = session.run(None, {\"image\": test_input})\n\nprint(f\"ONNX 出力形状: {outputs[0].shape}\")\nprint(\"ONNX モデル検証: OK\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Google Drive に保存（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## まとめ (v2)\n\n### v2 改善点\n- **OCR-B フォント**: MRZ標準フォントで合成データ生成\n- **データ拡張強化**: ノイズ、ブラー、回転、Perspective、輝度変動、JPEG圧縮\n- **AttentionCRNN**: Self-Attention で文字間依存関係を学習\n- **正則化強化**: Dropout 0.2、Weight Decay 0.01\n\n### 設定\n- **学習データ**: 合成 MRZ 画像 10,000枚 (OCR-B フォント)\n- **検証データ**: 合成 MRZ 画像 1,000枚\n- **モデルサイズ**: ~3.5 MB (Attention追加分)\n- **目標精度**: CER < 0.5%, Accuracy >= 99%\n\n### 次のステップ\n1. `mrz_crnn.onnx` をダウンロード\n2. WASM 変換 (`onnxruntime-web`)\n3. ブラウザでの推論テスト\n4. 精度が足りない場合: エポック増加 or データ量増加"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "- **学習データ**: 合成 MRZ 画像 10,000枚\n",
    "- **検証データ**: 合成 MRZ 画像 1,000枚\n",
    "- **モデルサイズ**: ~3 MB\n",
    "- **目標精度**: CER < 1%\n",
    "\n",
    "次のステップ:\n",
    "1. `mrz_crnn.onnx` をダウンロード\n",
    "2. WASM 変換 (`onnxruntime-web`)\n",
    "3. ブラウザでの推論テスト"
   ]
  }
 ]
}