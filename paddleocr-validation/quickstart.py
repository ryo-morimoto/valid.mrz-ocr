#!/usr/bin/env python3
"""
MRZ OCR æ¤œè¨¼ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

MIDV-500ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨PaddleOCR (Baiduè£½) ã‚’ä½¿ç”¨ã—ãŸæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
MIDV-500ã¯50ç¨®é¡ã®èº«åˆ†è¨¼æ˜æ›¸ã®500ãƒ“ãƒ‡ã‚ªã‚¯ãƒªãƒƒãƒ—ã‚’å«ã‚€ç ”ç©¶ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
PaddleOCRã¯45k+ starsã®å®Ÿç¸¾ã‚ã‚‹OSSã§ã€PP-OCRv4ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã€‚

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: https://arxiv.org/abs/1807.05786

Usage:
    uv run python quickstart.py              # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ã‚µãƒ³ãƒ—ãƒ«
    uv run python quickstart.py --samples 1  # 1ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    uv run python quickstart.py --samples 0  # å…¨ã‚µãƒ³ãƒ—ãƒ«
"""

import argparse
import json
import subprocess
from pathlib import Path


def install_dependencies():
    """
    å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

    pyproject.toml ã«å®šç¾©ã•ã‚ŒãŸä¾å­˜é–¢ä¿‚ã‚’ uv sync ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚
    """
    subprocess.check_call(["uv", "sync"])


def download_midv500_subset(output_dir: Path, max_docs: int = 3) -> Path:
    """
    MIDV-500ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ãƒãƒ¼ãƒˆã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ~5GBï¼‰ã§ã¯ãªãã€ãƒ‘ã‚¹ãƒãƒ¼ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’
    æŒ‡å®šæ•°ã ã‘ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¤œè¨¼æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ã€‚

    Args:
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        max_docs: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ‘ã‚¹ãƒãƒ¼ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3ï¼‰

    Returns:
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    """
    import zipfile

    dataset_dir = output_dir / "midv500"
    dataset_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ‘ã‚¹ãƒãƒ¼ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆEUä¸­å¿ƒï¼‰ã®ãƒªã‚¹ãƒˆ
    # æ³¨: USAã¯ passport card ã®ã¿ï¼ˆãƒ•ãƒ«ãƒ‘ã‚¹ãƒãƒ¼ãƒˆãªã—ï¼‰
    passport_docs = [
        "16_deu_passport_new",   # ãƒ‰ã‚¤ãƒ„ï¼ˆEUï¼‰
        "25_grc_passport",       # ã‚®ãƒªã‚·ãƒ£ï¼ˆEUï¼‰
        "28_hun_passport",       # ãƒãƒ³ã‚¬ãƒªãƒ¼ï¼ˆEUï¼‰
        "11_cze_passport",       # ãƒã‚§ã‚³ï¼ˆEUï¼‰
    ]

    # æŒ‡å®šæ•°ã«åˆ¶é™
    docs_to_download = passport_docs[:max_docs]

    print(f"Downloading {len(docs_to_download)} passport documents from MIDV-500...")
    print(f"Documents: {', '.join(docs_to_download)}")

    # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¯midv-500/dataset/ã«ã‚ã‚‹
    base_url = "ftp://smartengines.com/midv-500/dataset"

    def download_and_extract(doc_name: str) -> tuple[str, bool, str]:
        """å˜ä¸€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å±•é–‹"""
        zip_path = dataset_dir / f"{doc_name}.zip"
        doc_dir = dataset_dir / doc_name
        images_dir = doc_dir / "images"

        # æ—¢ã«å±•é–‹æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ï¼ˆimagesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼‰
        if images_dir.exists() and any(images_dir.glob("*.tif")):
            return (doc_name, True, "cached")

        try:
            # ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            url = f"{base_url}/{doc_name}.zip"
            print(f"  Downloading {doc_name} (~100MB)...")

            from tqdm import tqdm
            import urllib.request

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            with tqdm(unit="B", unit_scale=True, desc=f"    {doc_name}") as pbar:
                def reporthook(count, block_size, total_size):
                    if pbar.total is None and total_size > 0:
                        pbar.total = total_size
                    pbar.update(block_size)

                urllib.request.urlretrieve(url, zip_path, reporthook=reporthook)

            # å±•é–‹
            with zipfile.ZipFile(zip_path, "r") as zf:
                zf.extractall(dataset_dir)

            # ZIPãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            zip_path.unlink()

            return (doc_name, True, "downloaded")
        except Exception as e:
            return (doc_name, False, str(e))

    # é †æ¬¡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã®ãŸã‚ï¼‰
    results = []
    for doc in docs_to_download:
        doc_name, success, msg = download_and_extract(doc)
        results.append((doc_name, success, msg))
        if success:
            print(f"  âœ“ {doc_name} ({msg})")
        else:
            print(f"  âœ— {doc_name}: {msg}")

    success_count = sum(1 for _, s, _ in results if s)
    print(f"Download complete: {success_count}/{len(docs_to_download)} documents")

    return dataset_dir


def load_midv500_dataset(dataset_dir: Path) -> list[tuple[Path, str]]:
    """
    MIDV-500ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ç”»åƒã¨Ground Truth (MRZ) ã‚’èª­ã¿è¾¼ã‚€

    MIDV-500ã®Ground Truthæ§‹é€ :
    - ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆJSON (<doc_name>.json): ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å€¤ï¼ˆMRZå«ã‚€ï¼‰ã‚’æŒã¤
    - ãƒ•ãƒ¬ãƒ¼ãƒ JSON (TA01_01.jsonç­‰): åº§æ¨™æƒ…å ±ã®ã¿ï¼ˆMRZãƒ†ã‚­ã‚¹ãƒˆãªã—ï¼‰

    MRZã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆJSONã®field14ã¨field15ã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã€‚

    Returns:
        (ç”»åƒãƒ‘ã‚¹, MRZæ–‡å­—åˆ—) ã®ãƒªã‚¹ãƒˆ
    """
    results = []

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ç´¢
    for doc_dir in sorted(dataset_dir.iterdir()):
        if not doc_dir.is_dir():
            continue

        doc_name = doc_dir.name

        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆJSONã‹ã‚‰MRZ Ground Truthã‚’å–å¾—
        # å½¢å¼: ground_truth/<doc_name>.json
        template_json = doc_dir / "ground_truth" / f"{doc_name}.json"
        if not template_json.exists():
            continue

        try:
            with open(template_json) as f:
                gt_data = json.load(f)
        except (json.JSONDecodeError, OSError):
            continue

        # MRZè¡Œã‚’æŠ½å‡ºï¼ˆfield14=Line1, field15=Line2 ãŒä¸€èˆ¬çš„ï¼‰
        # TD3ãƒ‘ã‚¹ãƒãƒ¼ãƒˆã®å ´åˆ: 44æ–‡å­— x 2è¡Œ
        mrz_lines = []
        for field_name, field_data in gt_data.items():
            if not isinstance(field_data, dict):
                continue
            value = field_data.get("value", "")
            if not value:
                continue

            # MRZè¡Œã®ç‰¹å¾´: 44æ–‡å­—ã€å¤§æ–‡å­—è‹±æ•°å­—ã¨<ã®ã¿
            cleaned = value.replace(" ", "").upper()
            if len(cleaned) == 44 and "<" in cleaned:
                import re
                if re.match(r"^[A-Z0-9<]+$", cleaned):
                    # Yåº§æ¨™ã§ã‚½ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã€quadã®æƒ…å ±ã‚‚å–å¾—
                    quad = field_data.get("quad", [])
                    y_coord = quad[0][1] if quad else 0
                    mrz_lines.append((y_coord, cleaned))

        if len(mrz_lines) != 2:
            # TD3ãƒ‘ã‚¹ãƒãƒ¼ãƒˆä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        # Yåº§æ¨™ã§ã‚½ãƒ¼ãƒˆï¼ˆä¸Šã‹ã‚‰ä¸‹ã¸ï¼‰
        mrz_lines.sort(key=lambda x: x[0])
        mrz_text = "\n".join(line[1] for line in mrz_lines)

        # ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã‚’æ¢ã™
        # æ§‹é€ : images/<æ¡ä»¶ã‚³ãƒ¼ãƒ‰>/<ãƒ•ãƒ¬ãƒ¼ãƒ >.tif
        # æ¡ä»¶ã‚³ãƒ¼ãƒ‰: CA, CS, HA, HS, KA, KS, PA, PS, TA, TS
        images_dir = doc_dir / "images"
        if not images_dir.exists():
            continue

        # å„æ¡ä»¶ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒã‚’è¿½åŠ 
        # å„ªå…ˆé †: TA/TSï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ•´åˆ—ï¼‰> ãã®ä»–
        # PA/PSç­‰ã¯æ­ªã¿ãŒå¤§ããèªè­˜å›°é›£ãªãŸã‚å¾Œå›ã—
        priority_conditions = ["TA", "TS"]
        other_conditions = []

        for condition_dir in sorted(images_dir.iterdir()):
            if not condition_dir.is_dir():
                continue
            if condition_dir.name in priority_conditions:
                # å„ªå…ˆæ¡ä»¶ã¯å…ˆã«è¿½åŠ 
                for img_file in condition_dir.glob("*.tif"):
                    results.append((img_file, mrz_text))
            else:
                other_conditions.append(condition_dir)

        # ä»–ã®æ¡ä»¶ã¯å¾Œã§è¿½åŠ 
        for condition_dir in other_conditions:
            for img_file in condition_dir.glob("*.tif"):
                results.append((img_file, mrz_text))

    return results


def calculate_cer(prediction: str, ground_truth: str) -> float:
    """
    Character Error Rate ã‚’è¨ˆç®—

    Levenshteinè·é›¢ã‚’ä½¿ç”¨ã—ã¦æ–‡å­—å˜ä½ã®èª¤ã‚Šç‡ã‚’ç®—å‡ºã™ã‚‹ã€‚
    """
    from rapidfuzz.distance import Levenshtein

    if not ground_truth:
        return 0.0
    distance = Levenshtein.distance(prediction, ground_truth)
    return distance / len(ground_truth)


def is_mrz_line(text: str) -> bool:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãŒMRZè¡Œã‹ã©ã†ã‹ã‚’åˆ¤å®š

    MRZè¡Œã®ç‰¹å¾´:
    - TD3: 44æ–‡å­— (ãƒ‘ã‚¹ãƒãƒ¼ãƒˆ)
    - TD1: 30æ–‡å­— (IDã‚«ãƒ¼ãƒ‰)
    - TD2: 36æ–‡å­— (ãƒ“ã‚¶ç­‰)
    - ã€Œ<ã€ã‚’å«ã‚€
    - å¤§æ–‡å­—è‹±å­—ã€æ•°å­—ã€ã€Œ<ã€ã®ã¿ã§æ§‹æˆ
    """
    import re

    # ç©ºç™½ã‚’é™¤å»
    text = text.replace(" ", "").upper()

    # é•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆè¨±å®¹ç¯„å›²ã‚’æŒãŸã›ã‚‹: 28-46æ–‡å­—ï¼‰
    if not (28 <= len(text) <= 46):
        return False

    # ã€Œ<ã€ã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆMRZã®ç‰¹å¾´çš„ãªåŒºåˆ‡ã‚Šæ–‡å­—ï¼‰
    if "<" not in text:
        return False

    # MRZæ–‡å­—ã®ã¿ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆè‹±å¤§æ–‡å­—ã€æ•°å­—ã€<ï¼‰
    if not re.match(r"^[A-Z0-9<]+$", text):
        return False

    # ã€Œ<ã€ã®å‰²åˆãŒé«˜ã™ãã‚‹å ´åˆã¯é™¤å¤–ï¼ˆãƒã‚¤ã‚ºå¯¾ç­–ï¼‰
    filler_ratio = text.count("<") / len(text)
    if filler_ratio > 0.7:
        return False

    return True


def extract_mrz_from_ocr_result(ocr_result: list) -> str | None:
    """
    PaddleOCRçµæœã‹ã‚‰MRZãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    PaddleOCR 3.x API:
    - ocr_result: List[OCRResult]
    - OCRResult['rec_texts']: List[str] - èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ
    - OCRResult['dt_polys']: List[array] - åº§æ¨™ãƒãƒªã‚´ãƒ³

    ç¬¬1æ®µéš: OCRçµæœã‹ã‚‰MRZã‚‰ã—ã„è¡Œï¼ˆ44æ–‡å­—ã€<ã‚’å«ã‚€ï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    """
    if not ocr_result:
        return None

    # PaddleOCR 3.x å½¢å¼: OCRResult ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
    first_result = ocr_result[0]

    # è¾æ›¸é¢¨ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆOCRResult ã¯ dict-likeï¼‰
    rec_texts = first_result.get('rec_texts', []) if hasattr(first_result, 'get') else []
    dt_polys = first_result.get('dt_polys', []) if hasattr(first_result, 'get') else []

    if not rec_texts:
        return None

    mrz_lines = []

    for i, text in enumerate(rec_texts):
        if not text:
            continue

        # MRZè¡Œã‹ã©ã†ã‹åˆ¤å®š
        if is_mrz_line(text):
            # Yåº§æ¨™ï¼ˆç”»åƒå†…ã®ä½ç½®ï¼‰ã‚‚è¨˜éŒ²
            if i < len(dt_polys):
                poly = dt_polys[i]
                # ãƒãƒªã‚´ãƒ³ã®Yåº§æ¨™å¹³å‡
                y_coord = sum(p[1] for p in poly) / len(poly)
            else:
                y_coord = i  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

            mrz_lines.append((y_coord, text.replace(" ", "").upper()))

    if not mrz_lines:
        return None

    # Yåº§æ¨™ã§ã‚½ãƒ¼ãƒˆï¼ˆä¸Šã‹ã‚‰ä¸‹ã¸ï¼‰
    mrz_lines.sort(key=lambda x: x[0])

    # MRZè¡Œã‚’çµåˆ
    mrz_text = "\n".join(line[1] for line in mrz_lines)

    return mrz_text


def detect_mrz_region_opencv(image_path: str) -> tuple[int, int, int, int] | None:
    """
    OpenCVã§MRZé ˜åŸŸã‚’æ¤œå‡º

    ç¬¬2æ®µéšã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”»åƒå‡¦ç†ã§MRZé ˜åŸŸï¼ˆé»’æ–‡å­—ãŒå¯†é›†ã—ãŸé•·æ–¹å½¢ï¼‰ã‚’æ¤œå‡ºã™ã‚‹ã€‚
    MRZã¯é€šå¸¸ã€ç”»åƒä¸‹éƒ¨ã«ã‚ã‚Šã€æ¨ªé•·ã®çŸ©å½¢é ˜åŸŸã€‚
    """
    import cv2
    import numpy as np

    # ç”»åƒèª­ã¿è¾¼ã¿
    img = cv2.imread(image_path)
    if img is None:
        return None

    height, width = img.shape[:2]

    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # ç”»åƒä¸‹éƒ¨50%ã«ç„¦ç‚¹ï¼ˆMRZã¯é€šå¸¸ä¸‹éƒ¨ã«ã‚ã‚‹ï¼‰
    roi_start = int(height * 0.5)
    gray_roi = gray[roi_start:, :]

    # äºŒå€¤åŒ–ï¼ˆé»’æ–‡å­—ã‚’æ¤œå‡ºï¼‰
    _, binary = cv2.threshold(gray_roi, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼å‡¦ç†ã§æ–‡å­—é ˜åŸŸã‚’çµåˆ
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 3))
    dilated = cv2.dilate(binary, kernel, iterations=2)

    # è¼ªéƒ­æ¤œå‡º
    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # MRZã‚‰ã—ã„é ˜åŸŸã‚’æ¢ã™ï¼ˆæ¨ªé•·ã®çŸ©å½¢ï¼‰
    mrz_candidates = []
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)

        # MRZé ˜åŸŸã®æ¡ä»¶:
        # - å¹…ãŒç”»åƒå¹…ã®50%ä»¥ä¸Š
        # - ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãŒæ¨ªé•·ï¼ˆå¹…/é«˜ã• > 5ï¼‰
        # - é«˜ã•ãŒç”»åƒé«˜ã•ã®3-20%ç¨‹åº¦
        aspect_ratio = w / h if h > 0 else 0
        width_ratio = w / width
        height_ratio = h / (height - roi_start)

        if width_ratio > 0.5 and aspect_ratio > 5 and 0.03 < height_ratio < 0.3:
            # ROIåº§æ¨™ã‚’å…ƒç”»åƒåº§æ¨™ã«å¤‰æ›
            mrz_candidates.append((x, y + roi_start, w, h))

    if not mrz_candidates:
        return None

    # æœ€ã‚‚ä¸‹ã«ã‚ã‚‹å€™è£œã‚’é¸æŠï¼ˆMRZã¯æœ€ä¸‹éƒ¨ã«ã‚ã‚‹ï¼‰
    mrz_region = max(mrz_candidates, key=lambda r: r[1])

    # ãƒãƒ¼ã‚¸ãƒ³ã‚’è¿½åŠ 
    x, y, w, h = mrz_region
    margin_x = int(w * 0.02)
    margin_y = int(h * 0.1)

    x = max(0, x - margin_x)
    y = max(0, y - margin_y)
    w = min(width - x, w + 2 * margin_x)
    h = min(height - y, h + 2 * margin_y)

    return (x, y, w, h)


def extract_mrz_with_fallback(
    image_path: str, ocr_result: list, ocr_engine, debug: bool = False
) -> tuple[str, str]:
    """
    MRZãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰

    ç¬¬1æ®µéš: OCRçµæœã‹ã‚‰MRZãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    ç¬¬2æ®µéš: å¤±æ•—æ™‚ã€OpenCVã§é ˜åŸŸæ¤œå‡ºâ†’ã‚¯ãƒ­ãƒƒãƒ—â†’å†OCR

    Returns:
        (mrz_text, method): æŠ½å‡ºã•ã‚ŒãŸMRZãƒ†ã‚­ã‚¹ãƒˆã¨ä½¿ç”¨ã—ãŸæ–¹æ³•
    """
    import cv2

    # ç¬¬1æ®µéš: OCRçµæœã‹ã‚‰MRZãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
    mrz_text = extract_mrz_from_ocr_result(ocr_result)
    if mrz_text:
        return (mrz_text, "pattern_filter")

    if debug:
        print("[DEBUG] Pattern filter failed, trying OpenCV fallback...")

    # ç¬¬2æ®µéš: OpenCVã§é ˜åŸŸæ¤œå‡º
    region = detect_mrz_region_opencv(image_path)
    if region is None:
        if debug:
            print("[DEBUG] OpenCV MRZ detection failed")
        return ("", "detection_failed")

    if debug:
        print(f"[DEBUG] OpenCV detected MRZ region: {region}")

    # MRZé ˜åŸŸã‚’ã‚¯ãƒ­ãƒƒãƒ—
    x, y, w, h = region
    img = cv2.imread(image_path)
    cropped = img[y : y + h, x : x + w]

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦å†OCR
    import tempfile

    with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmp:
        cv2.imwrite(tmp.name, cropped)
        cropped_result = ocr_engine.predict(tmp.name)

        # ã‚¯ãƒ­ãƒƒãƒ—ç”»åƒã®OCRçµæœã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆPaddleOCR 3.x å½¢å¼ï¼‰
        lines = []
        if cropped_result and len(cropped_result) > 0:
            first_result = cropped_result[0]
            rec_texts = first_result.get('rec_texts', []) if hasattr(first_result, 'get') else []
            if debug:
                print(f"[DEBUG] Cropped OCR rec_texts: {rec_texts}")
            for text in rec_texts:
                if not text:
                    continue
                # MRZæ–‡å­—ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿
                cleaned = text.replace(" ", "").upper()
                if is_mrz_line(cleaned):
                    lines.append(cleaned)

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        Path(tmp.name).unlink(missing_ok=True)

    if lines:
        return ("\n".join(lines), "opencv_crop")

    return ("", "extraction_failed")


def validate_mrz_checksum(mrz_text: str) -> bool:
    """
    MRZãƒã‚§ãƒƒã‚¯ãƒ‡ã‚£ã‚¸ãƒƒãƒˆã‚’æ¤œè¨¼ï¼ˆTD3å½¢å¼ï¼‰

    ãƒ‘ã‚¹ãƒãƒ¼ãƒˆç•ªå·ã€ç”Ÿå¹´æœˆæ—¥ã€æœ‰åŠ¹æœŸé™ã®å„ãƒã‚§ãƒƒã‚¯ãƒ‡ã‚£ã‚¸ãƒƒãƒˆã‚’æ¤œè¨¼ã™ã‚‹ã€‚
    """
    lines = mrz_text.strip().split("\n")
    if len(lines) != 2 or len(lines[1]) != 44:
        return False

    line2 = lines[1]
    weights = [7, 3, 1]

    def calc_check(data: str) -> int:
        """é‡ã¿ä»˜ãåˆè¨ˆã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒ‡ã‚£ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—"""
        total = 0
        for i, char in enumerate(data):
            if char == "<":
                value = 0
            elif char.isdigit():
                value = int(char)
            elif char.isalpha():
                value = ord(char.upper()) - ord("A") + 10
            else:
                value = 0
            total += value * weights[i % 3]
        return total % 10

    try:
        # ãƒ‘ã‚¹ãƒãƒ¼ãƒˆç•ªå·ãƒã‚§ãƒƒã‚¯ (ä½ç½®0-9)
        if int(line2[9]) != calc_check(line2[0:9]):
            return False
        # ç”Ÿå¹´æœˆæ—¥ãƒã‚§ãƒƒã‚¯ (ä½ç½®13-19)
        if int(line2[19]) != calc_check(line2[13:19]):
            return False
        # æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯ (ä½ç½®21-27)
        if int(line2[27]) != calc_check(line2[21:27]):
            return False
        return True
    except (ValueError, IndexError):
        return False


def run_paddleocr_validation(images: list[tuple[Path, str]]) -> dict:
    """
    PaddleOCRã§æ¤œè¨¼å®Ÿè¡Œ

    å„ç”»åƒã«å¯¾ã—ã¦PaddleOCR (PP-OCRv4) ã‚’å®Ÿè¡Œã—ã€CER/LER/Checksumç‡ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    MRZé ˜åŸŸæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯:
    - ç¬¬1æ®µéš: OCRçµæœã‹ã‚‰MRZãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    - ç¬¬2æ®µéš: å¤±æ•—æ™‚ã€OpenCVã§é ˜åŸŸæ¤œå‡ºâ†’ã‚¯ãƒ­ãƒƒãƒ—â†’å†OCR
    """
    from paddleocr import PaddleOCR
    from tqdm import tqdm

    # PaddleOCRåˆæœŸåŒ–ï¼ˆè‹±èªãƒ¢ãƒ‡ãƒ«ï¼‰
    # æ–°APIã§ã¯use_angle_cls, use_gpu, show_logã¯éå¯¾å¿œ
    ocr = PaddleOCR(lang="en")

    results = {
        "total": len(images),
        "cer_sum": 0.0,
        "line_matches": 0,
        "checksum_passes": 0,
        "errors": [],
        "extraction_methods": {"pattern_filter": 0, "opencv_crop": 0, "failed": 0},
    }

    import tempfile
    import cv2

    for img_path, gt in tqdm(images, desc="PaddleOCR + MRZ extraction"):
        temp_jpg = None  # finallyã§ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç”¨
        try:
            # TIFãƒ•ã‚¡ã‚¤ãƒ«ã¯PaddleOCRãŒã‚µãƒãƒ¼ãƒˆã—ãªã„ãŸã‚ã€ä¸€æ™‚JPGã«å¤‰æ›
            img_path_str = str(img_path)

            if img_path_str.lower().endswith(".tif") or img_path_str.lower().endswith(".tiff"):
                img = cv2.imread(img_path_str)
                if img is None:
                    raise ValueError(f"Failed to load image: {img_path_str}")
                temp_file = tempfile.NamedTemporaryFile(suffix=".jpg", delete=False)
                temp_jpg = temp_file.name
                cv2.imwrite(temp_jpg, img)
                img_path_str = temp_jpg

            # PaddleOCRå®Ÿè¡Œ
            ocr_result = ocr.predict(img_path_str)

            # DEBUG: æœ€åˆã®æ•°æšã§çµæœå½¢å¼ã‚’è©³ç´°å‡ºåŠ›
            if len(results["errors"]) < 3:
                print(f"\n[DEBUG] Result type: {type(ocr_result)}")
                if isinstance(ocr_result, list) and len(ocr_result) > 0:
                    item = ocr_result[0]
                    print(f"[DEBUG] First item type: {type(item).__name__}")
                    # è¾æ›¸é¢¨ã‚¢ã‚¯ã‚»ã‚¹ã‚’è©¦è¡Œ
                    if hasattr(item, 'keys'):
                        print(f"[DEBUG] keys: {list(item.keys())}")
                    if hasattr(item, '__getitem__'):
                        try:
                            print(f"[DEBUG] item['rec_texts']: {item['rec_texts']}")
                            print(f"[DEBUG] item['dt_polys'] len: {len(item['dt_polys'])}")
                        except Exception as e:
                            print(f"[DEBUG] getitem error: {e}")
                    # å±æ€§ã‚’åˆ—æŒ™
                    attrs = [a for a in dir(item) if not a.startswith('_')]
                    print(f"[DEBUG] attributes: {attrs[:15]}")

            # MRZæŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
            # æœ€åˆã®æ•°æšã¯ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            is_debug = len(results["errors"]) < 3
            prediction, method = extract_mrz_with_fallback(
                str(img_path), ocr_result, ocr, debug=is_debug
            )

            # æŠ½å‡ºæ–¹æ³•ã‚’è¨˜éŒ²
            if method in ("pattern_filter", "opencv_crop"):
                results["extraction_methods"][method] += 1
            else:
                results["extraction_methods"]["failed"] += 1

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            cer = calculate_cer(
                prediction.replace("\n", "").replace(" ", ""),
                gt.replace("\n", "").replace(" ", ""),
            )
            results["cer_sum"] += cer

            if prediction.replace("\n", "") == gt.replace("\n", ""):
                results["line_matches"] += 1

            if validate_mrz_checksum(prediction):
                results["checksum_passes"] += 1

            # 5%ä»¥ä¸Šã®ã‚¨ãƒ©ãƒ¼ã¯è¨˜éŒ²
            if cer > 0.05:
                results["errors"].append(
                    {
                        "image": str(img_path),
                        "cer": cer,
                        "method": method,
                        "prediction": prediction[:50] + "...",
                        "ground_truth": gt[:50] + "...",
                    }
                )

        except Exception as e:
            results["errors"].append({"image": str(img_path), "error": str(e)})
            results["extraction_methods"]["failed"] += 1

        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if temp_jpg:
                Path(temp_jpg).unlink(missing_ok=True)

    # å¹³å‡è¨ˆç®—
    if results["total"] > 0:
        results["cer_avg"] = results["cer_sum"] / results["total"]
        results["ler"] = 1 - (results["line_matches"] / results["total"])
        results["checksum_rate"] = results["checksum_passes"] / results["total"]

    return results


def print_report(results: dict, tool_name: str):
    """
    æ¤œè¨¼çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›

    CERã€LERã€Checksumç‡ã€MRZæŠ½å‡ºæ–¹æ³•çµ±è¨ˆã€åˆæ ¼åˆ¤å®šã‚’è¡¨ç¤ºã™ã‚‹ã€‚
    """
    print("\n" + "=" * 60)
    print(f"MRZ OCR Validation Report - {tool_name}")
    print("=" * 60)

    print(f"\nTotal Samples: {results['total']}")

    # MRZæŠ½å‡ºæ–¹æ³•ã®çµ±è¨ˆã‚’è¡¨ç¤º
    methods = results.get("extraction_methods", {})
    if methods:
        print("\n--- MRZ Extraction Methods ---")
        print(f"  Pattern Filter: {methods.get('pattern_filter', 0)}")
        print(f"  OpenCV Crop:    {methods.get('opencv_crop', 0)}")
        print(f"  Failed:         {methods.get('failed', 0)}")

    cer = results.get("cer_avg", 0) * 100
    ler = results.get("ler", 0) * 100
    checksum = results.get("checksum_rate", 0) * 100

    cer_status = "âœ… PASS" if cer < 1 else "âŒ FAIL"
    ler_status = "âœ… PASS" if ler < 5 else "âŒ FAIL"
    checksum_status = "âœ… PASS" if checksum > 95 else "âŒ FAIL"

    print("\n--- Metrics ---")
    print(f"CER (avg):     {cer:.2f}%  {cer_status}")
    print(f"LER:           {ler:.2f}%  {ler_status}")
    print(f"Checksum Rate: {checksum:.2f}%  {checksum_status}")

    if results.get("errors"):
        print(f"\n--- Errors ({len(results['errors'])} samples) ---")
        for err in results["errors"][:5]:
            image_name = Path(err.get("image", "unknown")).name
            method = err.get("method", "unknown")
            error_msg = err.get("error")
            if error_msg is None:
                cer_value = err.get("cer", 0)
                error_msg = f"CER={cer_value:.2%} ({method})"
            print(f"  - {image_name}: {error_msg}")

    print("\n" + "=" * 60)

    # åˆ¤å®š
    if cer < 1 and ler < 5 and checksum > 95:
        print("ğŸ‰ OVERALL: PASS - Generic OCR meets requirements!")
        print("   â†’ Custom training is NOT required.")
    elif cer < 3:
        print("âš ï¸  OVERALL: MARGINAL - Consider preprocessing improvements")
        print("   â†’ Try: contrast normalization, skew correction")
    else:
        print("âŒ OVERALL: FAIL - Custom training is required.")
        print("   â†’ Proceed with PaddleOCR fine-tuning")

    print("=" * 60 + "\n")


def parse_args() -> argparse.Namespace:
    """
    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹

    --samples: å‡¦ç†ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ0=å…¨ä»¶ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ï¼‰
    --condition: æ¡ä»¶ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆTA,TSç­‰ï¼‰
    """
    parser = argparse.ArgumentParser(
        description="MRZ OCR validation using MIDV-500 and PaddleOCR"
    )
    parser.add_argument(
        "--samples",
        type=int,
        default=10,
        help="Number of samples to process (0=all, default=10)",
    )
    parser.add_argument(
        "--condition",
        type=str,
        default=None,
        help="Filter by condition code (e.g., TA, TS, PA). Comma-separated for multiple.",
    )
    return parser.parse_args()


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†

    1. ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    2. MIDV-500ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
    3. ç”»åƒã¨Ground Truthèª­ã¿è¾¼ã¿
    4. PaddleOCR (PP-OCRv4) æ¤œè¨¼
    """
    args = parse_args()

    print("=" * 60)
    print("MRZ OCR Validation Quick Start")
    print("Dataset: MIDV-500 (https://arxiv.org/abs/1807.05786)")
    print("=" * 60)

    # Step 1: ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    print("\n[1/4] Installing dependencies...")
    try:
        install_dependencies()
    except Exception as e:
        print(f"Warning: Some dependencies may not have installed: {e}")

    # Step 2: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆEU ãƒ‘ã‚¹ãƒãƒ¼ãƒˆï¼‰
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã® ../data ã‚’ä½¿ç”¨
    data_dir = Path(__file__).parent.parent / "data"
    print("\n[2/4] Downloading MIDV-500 passport subset (EU)...")
    dataset_dir = download_midv500_subset(data_dir, max_docs=4)

    # Step 3: ç”»åƒèª­ã¿è¾¼ã¿
    print("\n[3/4] Loading MRZ images with ground truth...")
    images = load_midv500_dataset(dataset_dir)
    print(f"Found {len(images)} MRZ images with ground truth")

    # æ¡ä»¶ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    if args.condition:
        condition_codes = [c.strip().upper() for c in args.condition.split(",")]
        images = [
            (path, gt) for path, gt in images
            if any(f"/{code}" in str(path).upper() for code in condition_codes)
        ]
        print(f"Filtered to {len(images)} images with conditions: {condition_codes}")

    if not images:
        print("No images found. Check dataset structure.")
        print(f"Expected structure: {dataset_dir}/<doc_type>/ground_truth/*.json")
        return

    # Step 4: æ¤œè¨¼å®Ÿè¡Œ
    print("\n[4/4] Running validation with PaddleOCR...")

    # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™ï¼ˆ--samples ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æŒ‡å®šï¼‰
    # 0 = å…¨ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
    max_samples = args.samples
    if max_samples > 0 and len(images) > max_samples:
        import random
        random.seed(42)  # å†ç¾æ€§ã®ãŸã‚ã‚·ãƒ¼ãƒ‰å›ºå®š
        images = random.sample(images, max_samples)
        print(f"Sampling {max_samples} images for quick validation")

    print(f"Running on {len(images)} samples...")
    print("(First run will download PP-OCRv4 model ~10MB)")
    results = run_paddleocr_validation(images)
    print_report(results, "PaddleOCR PP-OCRv4")


if __name__ == "__main__":
    main()
